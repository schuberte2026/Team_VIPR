{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4641f372",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "The purpose of this notebook to train a custom CNN model to classify paralyzed vs non-paralyzed vocal cords from ultrasound images.<br>\n",
    "\n",
    "0 - not paralyzed\n",
    "1 - paralyzed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f09e12",
   "metadata": {},
   "source": [
    "This cell contains the imports needed for the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a452fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a69be3",
   "metadata": {},
   "source": [
    "This cell creates a folder to save the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0eaadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "except OSError:\n",
    "    print('Error creating data directory.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106b7f15",
   "metadata": {},
   "source": [
    "Image filename meanings:\n",
    "- Healthy - regular healthy images of vocal cords\n",
    "- Healthy2 - split in half, and blended back together images of healthy vocal cords, hopefully helps alleviate a potential mdodel issue with artifacts in the synthetic images.\n",
    "- Leftpar - synthetic image, with the left side of the image stretched 50% vertically, right side is original image. \n",
    "- Rightpar - synthetic image, with the right side of the image stretched 50% vertically, left side is original image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6636c83",
   "metadata": {},
   "source": [
    "Defining a dataset class for our specific images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07697063",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_dir = img_dir  # Store image directory path\n",
    "        self.transform = transform\n",
    "\n",
    "        # List all image files in the directory\n",
    "        self.img_labels = []\n",
    "        print(f\"Looking in directory: {img_dir}\")\n",
    "\n",
    "        # Check if directory exists\n",
    "        if not os.path.exists(img_dir):\n",
    "            raise FileNotFoundError(f\"Directory {img_dir} not found\")\n",
    "\n",
    "        for file_name in os.listdir(img_dir):\n",
    "            # Debugging: print file names to see what's being processed\n",
    "#             print(f\"Found file: {file_name}\")\n",
    "            \n",
    "            # Check if it's a .png file and obtain class from filename\n",
    "            if file_name.endswith('.png'):  \n",
    "                label = None\n",
    "                if 'healthy' in file_name.lower():\n",
    "                    label = 0  # Healthy\n",
    "                elif 'healthy2' in file_name.lower():\n",
    "                    label = 0  # Healthy (modified)\n",
    "                elif 'leftpar' in file_name.lower():\n",
    "                    label = 1  # Left vocal cord paralysis\n",
    "                elif 'rightpar' in file_name.lower():\n",
    "                    label = 1  # Right vocal cord paralysis\n",
    "\n",
    "                if label is not None:\n",
    "                    self.img_labels.append((file_name, label))\n",
    "                else:\n",
    "                    print(f\"Skipping unknown file: {file_name}\")\n",
    "                \n",
    "        # Debugging: print number of labels found\n",
    "        print(f\"Found {len(self.img_labels)} .png files in the directory\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels[idx][0])\n",
    "        image = Image.open(img_path)  # Load the .png file using PIL\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = self.img_labels[idx][1]  # Extract label (0 for healthy, 1 for paralysis)\n",
    "        file_name = self.img_labels[idx][0]  # Extract file name\n",
    "        \n",
    "        return image, label, file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9629268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize to 256x256\n",
    "    transforms.Grayscale(num_output_channels=1),  # Ensure single grayscale channel\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize for grayscale images\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48611a2",
   "metadata": {},
   "source": [
    "Create the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7e347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "img_dir = '/data/ai_club/team_13_2024-25/VIPR/Data/training_images'\n",
    "dataset = CustomDataset(img_dir=img_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705c891c",
   "metadata": {},
   "source": [
    "Split the data and create dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fad6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and validation sets (80%-20% split)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoader for train and validation sets\n",
    "batch_size = 64  # You can adjust this batch size according to your needs\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed22f670",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6010cad",
   "metadata": {},
   "source": [
    "Setting up the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69b06f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a smaller custom CNN for binary classification\n",
    "class VIPRnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VIPRnet, self).__init__()\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # Grayscale (1 channel)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 32 * 32, 128),  # Adjusted for 256x256 input\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 1),  # Only 1 output neuron for binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x  # No sigmoid here; use BCEWithLogitsLoss instead\n",
    "\n",
    "# Model setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VIPRnet().to(device)\n",
    "\n",
    "# Loss & optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()  # For binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(model)  # Display model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0d7bf9",
   "metadata": {},
   "source": [
    "Train the model, with optional saving and loading of model checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4134f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a checkpoint to load if desired:\n",
    "load_model = False\n",
    "saved_checkpoint_name = \"VIPRnet-30-0.0776.pth\"\n",
    "checkpoint_path = os.path.join('models' ,saved_checkpoint_name)\n",
    "\n",
    "# Load existing model if checkpoint exists\n",
    "if load_model and os.path.exists(checkpoint_path):\n",
    "    print(f\"Loading model from {checkpoint_path}...\")\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1  # Continue from next epoch\n",
    "    print(f\"Resuming training from epoch {start_epoch}...\")\n",
    "else:\n",
    "    print('Starting training from scratch.')\n",
    "    start_epoch = 0  # Start from scratch\n",
    "    epoch_data_train = []\n",
    "    epoch_data_val = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "\n",
    "# Set desired number of epochs\n",
    "num_epochs = 50\n",
    "\n",
    "\n",
    "model.train() # set model to training mode\n",
    "for epoch in range(start_epoch,num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    # TRAINING PHASE\n",
    "    total_loss_train = 0\n",
    "    total_correct_train = 0\n",
    "    total_examples_train = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1} - TRAIN\", unit=\"batch\")\n",
    "    for batch_idx, (inputs, labels, _) in progress_bar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)  # Ensure shape [batch_size, 1]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss_train = criterion(outputs, labels)\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss_train += loss_train.item()\n",
    "        predictions_train = torch.round(torch.sigmoid(outputs))\n",
    "        \n",
    "        # Count correct predictions and total examples\n",
    "        total_correct_train += (predictions_train.squeeze() == labels.squeeze()).sum().item()\n",
    "        total_examples_train += labels.numel()\n",
    "        batch_count += 1\n",
    "\n",
    "    average_loss_train = total_loss_train / batch_count\n",
    "    average_accuracy_train = total_correct_train / total_examples_train\n",
    "    print(f\"TRAINING | Loss: {average_loss_train:.4f}, Accuracy: {average_accuracy_train:.2%}\")\n",
    "    epoch_data_train.append([epoch+1, average_loss_train, average_accuracy_train])\n",
    "    train_accuracies.append(average_accuracy_train)\n",
    "    \n",
    "    # VALIDATION PHASE\n",
    "    model.eval()\n",
    "    total_loss_val = 0\n",
    "    total_correct_val = 0\n",
    "    total_examples_val = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    progress_bar_val = tqdm(enumerate(val_loader), total=len(val_loader), desc=\"VAL\", unit=\"batch\")\n",
    "    for batch_idx, (inputs, labels, _) in progress_bar_val:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss_val = criterion(outputs.squeeze(), labels.float().squeeze())\n",
    "        total_loss_val += loss_val.item()\n",
    "        \n",
    "        predictions_val = torch.round(torch.sigmoid(outputs))\n",
    "        total_correct_val += (predictions_val.squeeze() == labels.squeeze()).sum().item()\n",
    "        total_examples_val += labels.numel()\n",
    "        batch_count += 1\n",
    "        \n",
    "    average_loss_val = total_loss_val / batch_count\n",
    "    average_accuracy_val = total_correct_val / total_examples_val\n",
    "    print(f\"VALIDATION | Loss: {average_loss_val:.4f}, Accuracy: {average_accuracy_val:.2%}\")\n",
    "    epoch_data_val.append([epoch+1, average_loss_val, average_accuracy_val])\n",
    "    val_accuracies.append(average_accuracy_val)\n",
    "    \n",
    "    # Optionally save model checkpoint here\n",
    "    save = True\n",
    "    checkpoint_path = f'models/VIPRnet_V3--B{batch_size}--E{epoch+1}--L{average_loss_train:.4f}.pth'\n",
    "    \n",
    "    if save:\n",
    "        # Save model checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': average_loss_train,\n",
    "            'train_data': epoch_data_train[-1],\n",
    "            'val_data': epoch_data_val[-1]\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Model saved to {checkpoint_path}\")\n",
    "    \n",
    "    # Switch back to train mode for next epoch\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e937947",
   "metadata": {},
   "source": [
    "Alternatively, train the model using DataParallel across multiple GPUs (minor speed increase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359fb295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for multiple GPUs and set up DataParallel\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs for training...\")\n",
    "    model = nn.DataParallel(model)  # Wrap model for multi-GPU training\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Choose a checkpoint to load if desired:\n",
    "load_model = False\n",
    "saved_checkpoint_name = \"VIPRnet-30-0.0776.pth\"\n",
    "checkpoint_path = os.path.join('models', saved_checkpoint_name)\n",
    "\n",
    "# Load existing model if checkpoint exists\n",
    "if load_model and os.path.exists(checkpoint_path):\n",
    "    print(f\"Loading model from {checkpoint_path}...\")\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1  # Continue from next epoch\n",
    "    print(f\"Resuming training from epoch {start_epoch}...\")\n",
    "else:\n",
    "    print('Starting training from scratch.')\n",
    "    start_epoch = 0  # Start from scratch\n",
    "    epoch_data_train = []\n",
    "    epoch_data_val = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "# Set desired number of epochs\n",
    "num_epochs = 50\n",
    "\n",
    "# Training loop\n",
    "model.train()  # Set model to training mode\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    # TRAINING PHASE\n",
    "    total_loss_train = 0\n",
    "    total_correct_train = 0\n",
    "    total_examples_train = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1} - TRAIN\", unit=\"batch\")\n",
    "    for batch_idx, (inputs, labels, _) in progress_bar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)  # Ensure shape [batch_size, 1]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)  # Model runs in parallel across all GPUs\n",
    "\n",
    "        loss_train = criterion(outputs, labels)\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss_train += loss_train.item()\n",
    "        predictions_train = torch.round(torch.sigmoid(outputs))\n",
    "        \n",
    "        # Count correct predictions and total examples\n",
    "        total_correct_train += (predictions_train.squeeze() == labels.squeeze()).sum().item()\n",
    "        total_examples_train += labels.numel()\n",
    "        batch_count += 1\n",
    "\n",
    "    average_loss_train = total_loss_train / batch_count\n",
    "    average_accuracy_train = total_correct_train / total_examples_train\n",
    "    print(f\"TRAINING | Loss: {average_loss_train:.4f}, Accuracy: {average_accuracy_train:.2%}\")\n",
    "    epoch_data_train.append([epoch+1, average_loss_train, average_accuracy_train])\n",
    "    train_accuracies.append(average_accuracy_train)\n",
    "\n",
    "    # VALIDATION PHASE\n",
    "    model.eval()  # Switch to evaluation mode\n",
    "    total_loss_val = 0\n",
    "    total_correct_val = 0\n",
    "    total_examples_val = 0\n",
    "    batch_count = 0\n",
    "\n",
    "    progress_bar_val = tqdm(enumerate(val_loader), total=len(val_loader), desc=\"VAL\", unit=\"batch\")\n",
    "    with torch.no_grad():  # No gradients for validation\n",
    "        for batch_idx, (inputs, labels, _) in progress_bar_val:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss_val = criterion(outputs.squeeze(), labels.float().squeeze())\n",
    "            total_loss_val += loss_val.item()\n",
    "\n",
    "            predictions_val = torch.round(torch.sigmoid(outputs))\n",
    "            total_correct_val += (predictions_val.squeeze() == labels.squeeze()).sum().item()\n",
    "            total_examples_val += labels.numel()\n",
    "            batch_count += 1\n",
    "\n",
    "    average_loss_val = total_loss_val / batch_count\n",
    "    average_accuracy_val = total_correct_val / total_examples_val\n",
    "    print(f\"VALIDATION | Loss: {average_loss_val:.4f}, Accuracy: {average_accuracy_val:.2%}\")\n",
    "    epoch_data_val.append([epoch+1, average_loss_val, average_accuracy_val])\n",
    "    val_accuracies.append(average_accuracy_val)\n",
    "\n",
    "    # Optionally save model checkpoint\n",
    "    save = True\n",
    "    checkpoint_path = f'models/ResNet18--B{batch_size}--E{epoch+1}--L{average_loss_train:.4f}.pth'\n",
    "\n",
    "    if save:\n",
    "        # Save model checkpoint - use .module when using DataParallel\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': average_loss_train,\n",
    "            'train_data': epoch_data_train[-1],\n",
    "            'val_data': epoch_data_val[-1]\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Model saved to {checkpoint_path}\")\n",
    "\n",
    "    # Switch back to train mode for next epoch\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5307046",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_accuracies)\n",
    "print(val_accuracies)\n",
    "print(num_epochs)\n",
    "num_epochs_list = []\n",
    "for i in range(epoch_data_train[-1][0]):\n",
    "    num_epochs_list.append(i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b31a7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(num_epochs_list, train_accuracies, label = 'train accuracies')\n",
    "plt.plot(num_epochs_list, val_accuracies, label = 'val accuracies')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bba6e4e",
   "metadata": {},
   "source": [
    "Test a model checkpoint on validation images from a the validation folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a800727",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the model checkpoint path\n",
    "model_path = 'VIPRnet_V3--B64--E50--L0.0152.pth'\n",
    "model_path = os.path.join('models', model_path)\n",
    "\n",
    "# Load saved checkpoint\n",
    "checkpoint = torch.load(model_path, map_location=device)  # Ensure it loads on the correct device\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()  # Switch to evaluation mode\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize to 256x256\n",
    "    transforms.Grayscale(num_output_channels=1),  # Ensure single grayscale channel\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize for grayscale images\n",
    "])\n",
    "\n",
    "# Load the image\n",
    "image_folder = \"\"  # Replace with your image folder path\n",
    "image_list = os.listdir(image_folder)\n",
    "for idx in range(len(image_list)):\n",
    "    image = Image.open(os.path.join(image_folder,image_list[idx]))\n",
    "    print('Filename:', image_list[idx])\n",
    "\n",
    "    # Apply the transformations\n",
    "    image = transform(image)\n",
    "    sample_image = transforms.ToPILImage()(image)\n",
    "    sample_image.show()\n",
    "    image = image.unsqueeze(0)  # Add a batch dimension (since the model expects a batch of images)\n",
    "\n",
    "    # Move the image to the same device as the model (GPU or CPU)\n",
    "    image = image.to(device)\n",
    "\n",
    "    # Make predictions\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = model(image)\n",
    "\n",
    "        # Apply sigmoid to get probabilities, then round to get binary output (0 or 1)\n",
    "        prediction = torch.round(torch.sigmoid(outputs))\n",
    "\n",
    "    # Print the predicted label\n",
    "    predictions = {0: 'Healthy', 1: 'Paralyzed'}\n",
    "    print(f\"Predicted label: {predictions[int(prediction.item())]}\\n\")  # 0 or 1 based on your binary classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9277a2",
   "metadata": {},
   "source": [
    "Get training graphs from folders of checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8740ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_balanced_peak_index(list1, list2, weight1=0.5, weight2=0.5):\n",
    "    \"\"\"\n",
    "    Finds the index where the values in both lists are highest in a balanced manner.\n",
    "\n",
    "    :param list1: First list of numerical values.\n",
    "    :param list2: Second list of numerical values.\n",
    "    :param weight1: Weight for the first list in the balancing formula.\n",
    "    :param weight2: Weight for the second list in the balancing formula.\n",
    "    :return: The index of the maximum balanced value.\n",
    "    \"\"\"\n",
    "    if len(list1) != len(list2):\n",
    "        raise ValueError(\"Both lists must be of the same length.\")\n",
    "\n",
    "    # Compute the balanced score for each index\n",
    "    balanced_scores = [(weight1 * list1[i] + weight2 * list2[i]) for i in range(len(list1))]\n",
    "\n",
    "    # Find the index of the maximum balanced score\n",
    "    max_index = balanced_scores.index(max(balanced_scores))\n",
    "\n",
    "    return max_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c884ba80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define checkpoint folder\n",
    "checkpoint_folder = 'VIPRnet_64'\n",
    "checklist = os.listdir(checkpoint_folder)\n",
    "\n",
    "# Extract data from saved checkpoints\n",
    "epochs = []\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "for checkpoint_path in checklist:\n",
    "    if checkpoint_path == '.ipynb_checkpoints':\n",
    "        continue  # skip checkpoints\n",
    "    checkpoint = torch.load(os.path.join(checkpoint_folder,checkpoint_path), map_location=torch.device('cpu'))\n",
    "    epochs.append(checkpoint['epoch'] + 1)\n",
    "#     train_losses.append(float((checkpoint_path.split('--')[-1])[1:-4]))\n",
    "    train_losses.append(checkpoint['train_data'][1])\n",
    "    train_accuracies.append(checkpoint['train_data'][2])\n",
    "    val_losses.append(checkpoint['val_data'][1])\n",
    "    val_accuracies.append(checkpoint['val_data'][2])\n",
    "    \n",
    "# Convert lists to numpy arrays and sort\n",
    "epochs = np.array(epochs)\n",
    "train_losses = np.array(train_losses)\n",
    "train_accuracies = np.array(train_accuracies)\n",
    "val_losses = np.array(val_losses)\n",
    "val_accuracies = np.array(val_accuracies)\n",
    "\n",
    "# Sort all by epoch number\n",
    "sorted_indices = np.argsort(epochs)\n",
    "epochs = epochs[sorted_indices]\n",
    "train_losses = train_losses[sorted_indices]\n",
    "train_accuracies = train_accuracies[sorted_indices]\n",
    "val_losses = val_losses[sorted_indices]\n",
    "val_accuracies = val_accuracies[sorted_indices]\n",
    "\n",
    "# Plot Training & Validation Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, train_losses, label='Training Loss', marker='o')\n",
    "plt.plot(epochs, val_losses, label='Validation Loss', marker='s')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training & Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig('VIPRnet_64_loss.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot Training & Validation Accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, train_accuracies, label='Training Accuracy', marker='o')\n",
    "plt.plot(epochs, val_accuracies, label='Validation Accuracy', marker='s')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training & Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig('VIPRnet_64_accuracy.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "highest_accuracy_idx = find_balanced_peak_index(list(train_accuracies), list(val_accuracies))\n",
    "best_model = checklist[highest_accuracy_idx]\n",
    "print(f'Highest scoring model is from epoch {epochs[highest_accuracy_idx]}. Path: {checklist[highest_accuracy_idx]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
